{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "**All references to Project, COS Bucket, and API keys must be replaced before running this notebook in your project. Eearch for \"replace\"**"}, {"metadata": {}, "cell_type": "markdown", "source": "<table style=\"border: none\" align=\"left\">\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>Build and Save a Sci-Kit Learn model to Watson Machine Learning (WML)</b></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr>\n</table>"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook walks you through these steps:\n \n- Access the data\n- Cleanse data for analysis\n- Explore data\n- Build a classification model\n- Save the model in the ML repository with associated meta data\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 1: Install and Import Required Libraries"}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import the pandas and seaborn for data handling and visualisation and datetime for date manipulations\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\npd.options.display.max_columns = 999\n%matplotlib inline\nsns.set(style=\"darkgrid\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import the required scikit-learn libraries\n\nimport numpy as np\nimport urllib3, requests, json\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy.stats import chi2_contingency,ttest_ind\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.impute import SimpleImputer\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 2: Import Data from Cloud Object Storage as Pandas Dataframe"}, {"metadata": {}, "cell_type": "markdown", "source": "<font color=red><b>ACTION:</b> Ensure that you have the correct bucket referenced below - it maybe easier to delete the next 4 cells and then use \"Insert to Code\" to add the three data sets</font>"}, {"metadata": {}, "cell_type": "code", "source": "# Instead of replacing bucket and COS API keys, we can also re-generate code. In that case we need to change the names of pandas data frame\n#customer - for the Mortgage_Customer.csv\n#property - for the Mortgage_Property.csv\n#default - for the Mortgage_Default.csv", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Connect to Cloud Object Storage to access the data\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_11c9f875c35a4c2c979b35121b858b43 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='replace-with-ibm-cos-api-key',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load the Customer Data\n\nbody = client_11c9f875c35a4c2c979b35121b858b43.get_object(Bucket='replace-with-your-cos-bucket',Key='Mortgage_Customer.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ncustomer = pd.read_csv(body)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load the Property Data\n\nbody = client_11c9f875c35a4c2c979b35121b858b43.get_object(Bucket='replace-with-your-cos-bucket',Key='Mortgage_Property.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nproperty = pd.read_csv(body)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Load the Default data\n\nbody = client_11c9f875c35a4c2c979b35121b858b43.get_object(Bucket='replace-with-your-cos-bucket',Key='Mortgage_Default.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndefault = pd.read_csv(body)\n\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# This is the project bucket, taken from the cell above, which can be used later if assets need to be moved from the project to a deployment space\nproject_bucket = \"replace-with-your-cos-bucket\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Check that the dataframes have the desired columns\n\nprint (\"Customer dataframe:\")\nprint (list(customer))\nprint (\"\")\nprint (\"Property dataframe:\")\nprint (list(property))\nprint (\"\")\nprint (\"Default dataframe:\")\nprint (list(default))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 3: Merge the Data Files"}, {"metadata": {}, "cell_type": "code", "source": "merged = pd.merge(pd.merge(customer, property, on='ID'),default,on='ID')\nmerged.head(3)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 4: Simple Data Preparation - Rename some columns and ensure correct data types \n#### Remove spaces from columns names"}, {"metadata": {}, "cell_type": "code", "source": "# Rename fields to remove spaces\nmerged.rename(columns={\n    \"Yrs at Current Address\":\"YearCurrentAddress\", \n    \"Yrs with Current Employer\":\"YearsCurrentEmployer\",\n    \"Number of Cards\":\"NumberOfCards\",\n    \"Creditcard Debt\":\"CCDebt\",\n    \"Loan Amount\":\"LoanAmount\"}, \n              inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Check the Data Types and correct any that require it"}, {"metadata": {}, "cell_type": "code", "source": "merged.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Loop through the Decimal (Float) fields and change to Integers\n\nfloat_col = merged.select_dtypes(include = ['float64']) # This will select float columns only\n# list(float_col.columns.values)\nfor col in float_col.columns.values:\n    merged[col] = merged[col].astype('int64')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "merged.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 5: Data Exploration\n\n1) Obtain some data shape summaries in terms of number of fields and records <br>\n2) Perform some exploratory analysis of distributions, scatterplots using appropriate visualisations libraries\n"}, {"metadata": {}, "cell_type": "code", "source": "print (\"There are \" + str(merged.shape[0]) + \" records and \" + str(merged.shape[1]) + \" fields in the dataset.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "g1 = sns.countplot(data=merged, x='MortgageDefault', order=merged.MortgageDefault.value_counts().index)\nplt.title('Mortgage Default Rates')\nplt.ylabel('Count of Default')\nplt.ylim(0, 800)\n#Add percentages to the graph\ntotal = float(len(merged)) #one person per row\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()/2.,\n            height + 1,\n            '{0:.0%}'.format(height/total),\n            ha=\"center\") \nplt.show()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sns.catplot(x=\"MortgageDefault\", y=\"YearCurrentAddress\",\n                 hue=\"Residence\", col=\"AppliedOnline\",\n                 data=merged, kind=\"bar\",\n                 height=7, aspect=.81,capsize=.05);", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sns.catplot(x=\"MortgageDefault\", y=\"SalePrice\",\n                 hue=\"Residence\", col=\"AppliedOnline\",\n                 data=merged, kind=\"bar\",\n                 height=7, aspect=.81,capsize=.05);", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sns.set(style=\"whitegrid\")\nplt.figure(figsize=(15,8))\ng = sns.lineplot(x=\"YearCurrentAddress\", y=\"CCDebt\", hue=\"Residence\",data=merged)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 6: Build the Sci-Kit Learn pipeline using a Random Forest model\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 6.1: Create the Input Data for Modelling"}, {"metadata": {}, "cell_type": "code", "source": "# Convert the Target/Label column to a numeric\n\nle = LabelEncoder()\nmerged.loc[:,'MortgageDefault']= le.fit_transform(merged.loc[:,'MortgageDefault'])\nmerged.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Check the values for MortgageDefault\n\nmerged.groupby(['MortgageDefault']).size()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Split the columns in to \"Input Features\" and \"Label\"\n\ny = np.float32(merged.MortgageDefault)\nx = merged.drop(['MortgageDefault','ID'], axis = 1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "list(x)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 6.2: Split the data in to Training & Test samples"}, {"metadata": {}, "cell_type": "code", "source": "# split the data to training and testing set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 6.3: Transform Input Fields -  LabelEncoding or OneHotEncoding for Catagorical & Scaled for Numerics"}, {"metadata": {}, "cell_type": "code", "source": "# Split the input features in to numeric/categorical features\n\nnumeric_features = ['Income','YearCurrentAddress','YearsCurrentEmployer','NumberOfCards','CCDebt','Loans','LoanAmount','SalePrice']\ncategorical_features = ['AppliedOnline','Residence','Location']\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# The definition of a Numeric transformation is shown here, but commented out in the pipeline creation below\n#   - the numeric transformation fills missing values (SimpleImputer) and then scales them to a standardised score (StandardScaler)\n# The definition of a Categorical tranformer is shown here - and used in the pipeline creation\n#   - the categorical transformer uses a OneHotEncoder but it could have been a label encoder\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Do not include numeric transforms to the pipeline creation and ensure that Non Categorical features are \"passed through\" and still avialable in their raw format\npreprocessor = ColumnTransformer(\n    transformers=[\n #       ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)],\n        remainder='passthrough')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 6.4: Define the classifier, the pipeline steps and then create the actual model"}, {"metadata": {}, "cell_type": "code", "source": "# Specify the classifier function to be used for the model creation\n\nclassifier_function = RandomForestClassifier()\n#classifier_function = DecisionTreeClassifier()\n#classifier_function = MLPClassifier()\n#classifier_function = LogisticRegression()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Define the pipeline as a series of steps\n\npipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', classifier_function)])  ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Create the model based upon the defined pipeline\n\nmodel = pipeline.fit(x_train,y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#print(model)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 7: Evaluate the Model and Check the Accuracy and Performance"}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 7.1: Look at the various Accuracy Measures"}, {"metadata": {}, "cell_type": "code", "source": "# call pipeline.predict() on your X_test data to make a set of test predictions which are written to series y_prediction\n\ny_prediction = pipeline.predict(x_test)\ny_probability = pipeline.predict_proba(x_test)\n\n# Evaluate the model using sklearn.classification_report()\nreport = sklearn.metrics.classification_report(y_test, y_prediction )\naccuracy = sklearn.metrics.accuracy_score(y_test, y_prediction )\nprint(report)\nprint(\"Overall Model Accuracy: \" + str(accuracy))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 7.2: Score the Test Data through the Model Pipeline and view the Actual & Predicted Results"}, {"metadata": {}, "cell_type": "code", "source": "#Reset the index on the x_train data so that the join will match record by record and not require a key\nx_test.reset_index(drop=True, inplace=True)\n\n#Write the Actual and Predicted Mortgage Default values in to dataframes \ny_test_df = pd.DataFrame(y_test,columns=['MortgageDefault'])\ny_pred_df = pd.DataFrame(y_prediction,columns=['Pred Default'])\ny_prob_df = pd.DataFrame(y_probability,columns=['Prob Non-Default','Prob Default'])\n\n# Combine the three dataframes by index value rather than key field\nscored_df = pd.concat([x_test, y_test_df, y_pred_df, y_prob_df], axis=1)\nscored_df.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 8: Understand the Model which has been created\nFeature Importance must be calculated based upon the transformed data and not the original data - therefore the final fields in the model are obtained by accessing them from the first step of the pipeline (post transformation)"}, {"metadata": {}, "cell_type": "code", "source": "# Obtain the fields submitted to the model after the first step of the pipeline (post transformation)\nonehot_columns = pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features)\n\n\n# Create a pandas series which contains both the transformed feature list and the calculated importance based upon the model fit\nX_values = preprocessor.fit_transform(x_train)\ndf_from_array_pipeline = pd.DataFrame(X_values, columns = numeric_features + list(onehot_columns) )\nfeature_imp_series = pd.Series(data= pipeline.named_steps['classifier'].feature_importances_, index = np.array(numeric_features + list(onehot_columns)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Convert the pandas series to a pandas dataframe - rename the columns and sort in descending order of Feature Importance\n\nfeature_imp = feature_imp_series.to_frame()\nfeature_imp['Feature'] = feature_imp.index\nfeature_imp = feature_imp.rename(columns = {0: 'Importance'})\nfeature_imp = feature_imp.sort_values(by=['Importance'], ascending=False)\nfeature_imp.head(25)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualise the Feature Importance in descending order\n\nsns.factorplot(y=\"Feature\",x=\"Importance\", data=feature_imp,kind=\"bar\",palette=\"Blues\",size=6,aspect=2)\nplt.title('Mortgage Default - Feature Importance')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Step 9: Save Model to  the Project"}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 9.1: Obtain Credentials for WML and initiate the WML Client API, then choose the Model Name"}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\n# IMPORTANT\n# Replace with your Cloud API key and location\napi_key = 'replace-with-your-cloud-api-key'\nlocation = 'https://us-south.ml.cloud.ibm.com'  # For example, Dallas location is 'https://us-south.ml.cloud.ibm.com'\n\n\nwml_credentials = {\n    \"apikey\": api_key,\n    \"url\": location\n}\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.set.default_project(pc.projectID)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 9.2: Store the Model to the Project"}, {"metadata": {}, "cell_type": "code", "source": "# Provide metadata and save the model into the repository. After running this cell, the model will be displayed in the Assets view\n\n# Model Metadata\nmodel_name = 'mortgage_default_model'\nsoftware_spec_uid = client.software_specifications.get_uid_by_name('default_py3.8')\n\nmetadata = {\n    client.repository.ModelMetaNames.NAME: model_name,\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n    client.repository.ModelMetaNames.TYPE: \"scikit-learn_0.23\"\n}\n\nstored_model_details = client.repository.store_model(pipeline,\n                                               meta_props=metadata,\n                                               training_data=x_train,\n                                               training_target=y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 9.3: Save Model in the Project (as an asset) - optional\nWhile we recommend that models are saved using the WML API, there may be cases when the models may need to be saved as files. Two typical reasons for using this approach are:\n1. Model framework is not supported by Watson Studio. You can find the list of supported frameworks in documentation: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=functions-supported-deployment-frameworks\n2. A customer has an established deployment process which works with exported model files. \n\n*In this notebook we demontrate how to save the model to the project (it will be displayed under Data Assets). You can also save the model file to any storage type - Storage Volume (shared file system), Git, Object Storage, etc.*"}, {"metadata": {}, "cell_type": "code", "source": "model_name = 'mortgage_default_model'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pickle\nfrom io import BytesIO\n\n# Save the model to working directory. Pickle is one of the most frequently used options for saving Python models, but you can also use other approaches to save the model file. \n# Modify model name to make it easier to distinguish between the model saved with WML and regular save file.\nmodel_name = model_name + '_custom'\n\n# save model object as in-memory bytes buffer\nbuffer = BytesIO()\npickle.dump(model, buffer, pickle.HIGHEST_PROTOCOL)\nbuffer.seek(0)\n\nproject.save_data(model_name, buffer, overwrite=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "---------------------------------------------------------------------------------\n<u>Author Information:</u><br>\n**Stephen Groves** and **Elena Lowery** <br/>\n<i> Data Science & AI Technical Sales, IBM </i><br>\n9 October 2020, updated in December 2021"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}