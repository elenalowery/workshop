{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "**Important: All references to project, COS bucket, and API keys must be replaced before running this notebook in your project. Search this notebook for \"replace\"**"}, {"metadata": {"id": "fdbcec80-d59f-42ad-a6ab-4366934f63ca"}, "cell_type": "markdown", "source": "### Scoring a Custom Model\nWe use a generic term *custom model* to describe a model that was saved into a project as a file using a Python API and not WML API. \n\nWhile we recommend that models are saved using the WML API and deployed as *models* in Deployment Spaces, there may be cases when the models may need to be saved as files. Three typical reasons for using this approach are:\n1. A model was trained outside of Watson Studio, but it needs to be deployed in Deployment Spaces\n2. Framework used by the model is not supported by default in Watson Studio, that's why WML API can't be used. You can find the list of supported frameworks in documentation: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=functions-supported-deployment-frameworks\n3. A customer has an established deployment process which works with exported model files. \n\n*In this notebook we demontrate how to load the model from the project (the model is available in Data Assets). You can also load the model file from any storage type - Storage Volume (shared file system), Git, Object Storage, etc.*"}, {"metadata": {"id": "b6b3d0e0-318f-4a42-8980-435776b92659"}, "cell_type": "markdown", "source": "### Step 1: Load the model from Data Assets"}, {"metadata": {"id": "aefbfebfdf2545e9b7c831187b56452d"}, "cell_type": "code", "source": "# Get the file - the file name was specified in the MortgageDefault-sklearn notebook\nbuffer = project.get_file(\"mortgage_default_model_custom\")\ntype(file)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pickle\n\n# load the model from buffer\nloaded_model = pickle.load(buffer)\n\n# Verify that the model was serialized into the right object\ntype(loaded_model)", "execution_count": null, "outputs": []}, {"metadata": {"id": "16982f7222fe4aa8a46591be24073372"}, "cell_type": "markdown", "source": "### Step 2: Load and preview data that will be scored"}, {"metadata": {}, "cell_type": "code", "source": "# Instead of repacing the COS API key and bucket, you can regenerate code and replace the pandas data frame name. Change it to \n#scoring_input_data", "execution_count": null, "outputs": []}, {"metadata": {"id": "8b3bf7b4e1e545a78e06676d6ababc05"}, "cell_type": "code", "source": "import os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\n\nif os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n    endpoint_1e498447d5f74cd6b90b92b35bb6514e = 'https://s3.us.cloud-object-storage.appdomain.cloud'\nelse:\n    endpoint_1e498447d5f74cd6b90b92b35bb6514e = 'https://s3.private.us.cloud-object-storage.appdomain.cloud'\n\nclient_1e498447d5f74cd6b90b92b35bb6514e = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='replace-with-your-cos-api-ky',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url=endpoint_1e498447d5f74cd6b90b92b35bb6514e)\n\nbody = client_1e498447d5f74cd6b90b92b35bb6514e.get_object(Bucket='replace-with-your-cos-bucket')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\nscoring_input_data = pd.read_csv(body)\nscoring_input_data.head()\n", "execution_count": null, "outputs": []}, {"metadata": {"id": "ef88521918134da6a5106283ce92f95b"}, "cell_type": "markdown", "source": "### Step 3: Score data"}, {"metadata": {"id": "feabb588e07f4ef48e0cf9d3980ef319"}, "cell_type": "code", "source": "# Get scoring results and probability of churn (prediciton of T=True)\nscoring_results = loaded_model.predict(scoring_input_data)\nscoring_prob_T = loaded_model.predict_proba(scoring_input_data)[:, 1]", "execution_count": null, "outputs": []}, {"metadata": {"id": "c4a607e8d5ca4c508f715e785aeac1db"}, "cell_type": "code", "source": "# Scoring results are in an array format - the order is the same as the order of records in input, \n# and that's why we can append the input data with results based on order\nprint(scoring_results)\n\nscoring_input_data['prediction']=scoring_results\nscoring_input_data['probability_T'] = scoring_prob_T", "execution_count": null, "outputs": []}, {"metadata": {"id": "c3015c9ed11e4828876a254d289a6fc1"}, "cell_type": "code", "source": "scoring_output_data=scoring_input_data\n# Display the data frame that we will write to a file\nscoring_output_data", "execution_count": null, "outputs": []}, {"metadata": {"id": "a2d1e45e-4ad4-434c-bb8c-e31da639186c"}, "cell_type": "markdown", "source": "### Step 4: Save scoring results to Data Assets\nIn this example we save scoring results to Data Assets. You can also save scoring results to other data sources - Storage Volume (shared file system), Git, Object Storage, databases, Hadoop, etc. "}, {"metadata": {}, "cell_type": "code", "source": "project.save_data(\"ScoringOutput_Custom_Model.csv\", scoring_output_data.to_csv(index=False))", "execution_count": null, "outputs": []}, {"metadata": {"id": "d5bcb1b0-b3c6-4b0c-89c5-ec50e11c522e"}, "cell_type": "markdown", "source": "**Author:**  Elena Lowery and Catherine Cao <br/>\n**Date:**  December 2021"}, {"metadata": {"id": "513fbf3303ae463c9cf25ae02d6eebeb"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}