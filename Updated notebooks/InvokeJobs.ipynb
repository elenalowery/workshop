{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71aafcb6e646430a8dc26efb2c1c4e1e"
   },
   "source": [
    "# Invoking Jobs: Examples\n",
    "This notebook provides an example of invoking Jobs that are defined in the project. \n",
    "\n",
    "More information about the API (**Watson Data API**) that's used in this notebook is available in product documentation: https://cloud.ibm.com/apidocs/watson-data-api-cpd  It is also possible to invoke jobs using *cpdctl* API, which we demonstrate in another notebook. \n",
    "\n",
    "APIs are typically used for *automation* and *orchestration*. We implemented code in a notebook that's a part of a Watson Studio project for ease of demonstration. Sample Python code can also be saved as a Python script and executed from an external environment (such as a CI/CD platform). \n",
    "\n",
    "Since we use the terms automation and orchestration, let's define them. **Automation** is invoking the process programatically without human interaction, usually based on a schedule or a trigger. **Orchestration** is combining multiple steps into a single process.  \n",
    "\n",
    "Any type of asset that can be configured to run as a *Job* in a *Watson Studio project* can be used with this API. Current version of Cloud Pak for Data supports jobs for:\n",
    "- Notebooks\n",
    "- Python scripts\n",
    "- R scripts\n",
    "- Modeler flows\n",
    "- Refinery flows\n",
    "\n",
    "There are several use cases for automation and orchestration of jobs. Here are a few examples:\n",
    "- Automate and/or orchestrate data preparation (implemented in scripts, flows, or notebooks) based on a schedule or an external trigger\n",
    "- Automate and/or orchestrate model retraining (implemented in scripts, flows, or notebooks) based on a schedule or an external trigger\n",
    "- Automate and/or orchestrate testing and deployment of data science assets into *Deployment Spaces*. \n",
    "\n",
    "In this notebook we will show a simple example which you can expand to fit your use cases. \n",
    "\n",
    "*Note: This notebook has been written and tested in Cloud Pak for Data Hybrid Cloud.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "147f895e27ae4f828be3432e452ee225"
   },
   "source": [
    "## Step 1: Manually create a Job\n",
    "While it's possible to create a job with an API, we think that in most scenarios jobs will be created and tested manually, and then used with automation/orchestration.\n",
    "\n",
    "Create a Job for any asset that you want to test. For example, the *Predict_Customer_Churn* notebook that's included in this project. This notebook creates a model and saves it in the project,  When you run the notebook as a batch job, it will perform the same steps as in interactive mode - it will build and save a model.\n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "1. Open the notebook in *Edit* mode and save a version (look for *Versions* icon in the rigth top toolbar). Jobs require versioning of notebooks and flows.\n",
    "2. Create a job\n",
    "3. Test the job. In addition to the successful run, you should see a model created under the Models section of the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2270145232734ae7acd567860348c7c9"
   },
   "source": [
    "## Step 2: Invoke the Job\n",
    "To construct the API call, we will need to get the following information:\n",
    "1. *Authorizaton token*: this token is required for all calls to Cloud Pak for Data API\n",
    "2. *Project id*: needed as a paramter for the job invocation REST request\n",
    "3. *Asset id*: needed as a parameter for the job invocation REST request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41e5ae9e75084de4843b02ce355c9f50"
   },
   "outputs": [],
   "source": [
    "# If you're running in a Watson Studio project, the token is available as a local variable\n",
    "#token = os.environ['USER_ACCESS_TOKEN']\n",
    "\n",
    "#In this notebook we will demonstrate retrieving a token via API, which will be required for code running outside of Cloud Pak for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07c85a1bae184ceaa2553e7856c530fd"
   },
   "outputs": [],
   "source": [
    "# Define variables that need to be changed or reused\n",
    "\n",
    "# TO DO: change to the hostname (and port, if defined) of your cluster\n",
    "\n",
    "# If using a market cluster in North America (in TEC), the value should be 'https://ibm-nginx-svc.cpdmkt.svc' (this value is the same for ALL clusters)\n",
    "# For all other clusters, use the CPD URL that end with .oi, for example, 'https://cpdmkt-cpd-cpdmkt.apps.cpd.12-181-164-84.nip.io'\n",
    "cpd_hostname = \"***\"\n",
    "\n",
    "# TO DO: change to userid and password that exists in the CPD cluster. These credentials will be used to generate a token\n",
    "username = \"***\"\n",
    "password = \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4596b5602c9443a5894d5059d8509801"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "data = '{\"username\":\\\"' + username + '\\\",\"password\":\\\"' + password + '\\\"}\\''\n",
    "\n",
    "# Construct the request URL\n",
    "requestURL = cpd_hostname + \"/icp4d-api/v1/authorize\"\n",
    "\n",
    "response = requests.post(requestURL, headers=headers, data=data, verify=False)\n",
    "\n",
    "responseContent = response.content\n",
    "token = json.loads(responseContent)['token']\n",
    "\n",
    "# Print token just for a demo - remove in production\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3b0f7d0cfe4461596978157a80fcc2b"
   },
   "outputs": [],
   "source": [
    "#Next, we will get the project id. We can use the watson-studio-lib library to perform this task. \n",
    "\n",
    "# Import the lib\n",
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "wslib = access_project_or_space()\n",
    "\n",
    "# Get project id\n",
    "projectID = wslib.here.get_ID()\n",
    "print(projectID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4a3dfa71d054b8da8af876e3a4eb815"
   },
   "outputs": [],
   "source": [
    "# This funciton is useful for looking up the value for the \"Job\" asset type, which we will use in the next cell\n",
    "wslib.assets.list_asset_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55be665f51a84a428ddf4d1445ef102f"
   },
   "outputs": [],
   "source": [
    "# Get the Job id\n",
    "wslib.assets.list_assets(\"job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "054e51a95e4748d9bea08b414115fb53"
   },
   "outputs": [],
   "source": [
    "# Manually look up the the asset_id for the Notebook Job that you created and save it in a variable. It will be used to construct REST request URL. \n",
    "# Make sure to get the ID for the Notebook Job, not Notebook Job Run. \n",
    "jobID = \"47181527-1052-4b5e-9a1c-edc70fc335ed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33e279a4-b633-4960-a1de-de3c40b1eecd"
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "     'Authorization': 'Bearer ' + token,\n",
    "     'accept': 'application/json',\n",
    "     'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# This JSON format will work even if the Job doesn't have parameters (like the sample notebook Job we configured in Step 1)\n",
    "\n",
    "dataDict = {\n",
    "   \"job_run\": {\n",
    "        \"configuration\": {\n",
    "            \"env_variables\": [\n",
    "                \"variable1=test1\",\n",
    "                \"variable2=test2\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data = json.dumps(dataDict)\n",
    "print(headers)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d237c6c344644eb888142ddd9eda75a4"
   },
   "outputs": [],
   "source": [
    "#Construct the URL for invoking the job. We are using this REST endpoint: https://cloud.ibm.com/apidocs/watson-data-api-cpd#job-runs-create\n",
    "url =  cpd_hostname + \"/v2/jobs/\" + jobID + \"/runs?project_id=\" + projectID\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39dec790d66347a38c68721ab1a3218c"
   },
   "outputs": [],
   "source": [
    "response = requests.post(url, headers=headers, data=data, verify=False)\n",
    "\n",
    "responseContent = response.content\n",
    "print(responseContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcc7226e1df647cfa16ac0d8233b7caf"
   },
   "outputs": [],
   "source": [
    "# If we want to check the job status, we need to get the run ID, which is called asset_id\n",
    "runID = json.loads(responseContent)['metadata']['asset_id']\n",
    "print(runID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d86ac301cf194fea866955769bcb574a"
   },
   "outputs": [],
   "source": [
    "url = cpd_hostname + \"/v2/jobs/\" + jobID + \"/runs/\" + runID + \"?project_id=\" + projectID\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cb77f339be234c0a9097e38e2af83732"
   },
   "outputs": [],
   "source": [
    "response = requests.get(url, headers=headers, verify=False)\n",
    "responseContent = response.content\n",
    "print(responseContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2795a54f6b93475187e6ca5fc1bf95aa"
   },
   "outputs": [],
   "source": [
    "# Job Status is reported in variable \"state\"\n",
    "jobStatus = json.loads(responseContent)['entity']['job_run']['state']\n",
    "print(jobStatus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "747a8ebde6874327855ccd00c28ec8b3"
   },
   "source": [
    "<span style=\"color:red\">Important Note: Check the Jobs tab of your project. You should now see a running job</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a06af01dca04e6d91967e1117906f4d"
   },
   "outputs": [],
   "source": [
    "# Status look up can also be implemented in a loop. This is useful when you need to invoke a 2nd job after the completion of the first one\n",
    "import time\n",
    "\n",
    "while jobStatus == \"Starting\" or jobStatus == \"Running\":\n",
    "  response = requests.get(url, headers=headers, verify=False)\n",
    "  responseContent = response.content\n",
    "  jobStatus = json.loads(responseContent)['entity']['job_run']['state']\n",
    "  print(jobStatus)\n",
    "# Wait for 30 seconds before checking status again\n",
    "  time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e42a6d9887cf4399b36b205061573503"
   },
   "outputs": [],
   "source": [
    "# Here you can add the call to the 2nd step in your orchestration workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea828108b7304f5fbcbb0de7e2ff2fda"
   },
   "source": [
    "**Written by: Elena Lowery, April 2022**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
