{"cells": [{"metadata": {"id": "71aafcb6e646430a8dc26efb2c1c4e1e"}, "cell_type": "markdown", "source": "# Invoking Jobs: Examples\nThis notebook provides an example of invoking Jobs that are defined in the project. \n\nMore information about the APIs (**Watson Data API**) that are used in this notebook is available in product documentation: https://cloud.ibm.com/apidocs/watson-data-api-cpd\n\nAPIs are typically used for *automation* and *orchestration*. We implemented code in a notebook that's a part of a Watson Studio project for ease of demonstration. Sample Python code can also be saved as a Python script and executed from an external environment (such as a CI/CD platform). \n\nSince we use the terms automation and orchestration, let's define them. **Automation** is invoking the process programatically without human interaction, usually based on a schedule or a trigger. **Orchestration** is combining multiple steps into a single process.  \n\nAny type of asset that can be configured to run as a *Job* in a *Watson Studio project* can be used with this API. Current version of Cloud Pak for Data supports jobs for:\n- Notebooks\n- Python scripts\n- R scripts\n- Modeler flows\n- Refinery flows\n\nThere are several use cases for automation and orchestration of jobs. Here are a few examples:\n- Automate and/or orchestrate data preparation (implemented in scripts, flows, or notebooks) based on a schedule or an external trigger\n- Automate and/or orchestrate model retraining (implemented in scripts, flows, or notebooks) based on a schedule or an external trigger\n- Automate and/or orchestrate testing and deployment of data science assets into *Deployment Spaces*. \n\nIn this notebook we will show a simple example which you can expand to fit your use cases. \n\n*Note: This notebook has been written and tested in Cloud Pak for Data Hybrid Cloud.*"}, {"metadata": {"id": "147f895e27ae4f828be3432e452ee225"}, "cell_type": "markdown", "source": "## Step 1: Manually create a Job\nWhile it's possible to create a job with an API, we think that in most scenarios jobs will be created and tested manually, and then used with automation/orchestration.\n\nCreate a Job for any asset that you want to test. For example, the *Predict_Customer_Churn* notebook that's included in this project. This notebook creates a model and saves it in the project,  When you run the notebook as a batch job, it will perform the same steps as in interactive mode - it will build and save a model.\n\nComplete the following steps:\n\n1. Open the notebook in *Edit* mode and save a version (look for *Versions* icon in the rigth top toolbar). Jobs require versioning of notebooks and flows.\n2. Create a job\n3. Test the job. In addition to the successful run, you should see a model created under the Models section of the project. "}, {"metadata": {"id": "2270145232734ae7acd567860348c7c9"}, "cell_type": "markdown", "source": "## Step 2: Invoke the Job\nTo construct the API call, we will need to get the following information:\n1. *Authorizaton token*: this token is required for all calls to Cloud Pak for Data API\n2. *Project id*: needed as a paramter for the job invocation REST request\n3. *Asset id*: needed as a parameter for the job invocation REST request"}, {"metadata": {"id": "41e5ae9e75084de4843b02ce355c9f50"}, "cell_type": "code", "source": "# If you're running in a Watson Studio project, the token is available as a local variable\n#token = os.environ['USER_ACCESS_TOKEN']\n\n#In this notebook we will demonstrate retrieving a token via API, which will be required for code running outside of Cloud Pak for Data", "execution_count": null, "outputs": []}, {"metadata": {"id": "07c85a1bae184ceaa2553e7856c530fd"}, "cell_type": "code", "source": "# Define variables that need to be changed or reused\n\n# TO DO: change to the hostname (and port, if defined) of your cluster\n\n# If using a market cluster in North America (in TEC), the value should be 'https://ibm-nginx-svc.cpdmkt.svc' (this value is the same for ALL clusters)\n# For all other clusters, use the CPD URL that end with .oi, for example, 'https://cpdmkt-cpd-cpdmkt.apps.cpd.12-181-164-84.nip.io'\ncpd_hostname = \"***\"\n\n# TO DO: change to userid and password that exists in the CPD cluster. These credentials will be used to generate a token\nusername = \"***\"\npassword = \"***\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "4596b5602c9443a5894d5059d8509801"}, "cell_type": "code", "source": "import requests\nimport json\n\nheaders = {\n    'Content-Type': 'application/json',\n}\n\ndata = '{\"username\":\\\"' + username + '\\\",\"password\":\\\"' + password + '\\\"}\\''\n\n# Construct the request URL\nrequestURL = cpd_hostname + \"/icp4d-api/v1/authorize\"\n\nresponse = requests.post(requestURL, headers=headers, data=data, verify=False)\n\nresponseContent = response.content\ntoken = json.loads(responseContent)['token']\n\n# Print token just for a demo - remove in production\nprint(token)", "execution_count": null, "outputs": []}, {"metadata": {"id": "b3b0f7d0cfe4461596978157a80fcc2b"}, "cell_type": "code", "source": "#Next, we will get the project id. We can use the project-lib library to perform this task. \n\n# Import the lib\nfrom project_lib import Project\nproject = Project.access()\n\nprojectMetadata = project.get_metadata()\n\n# Let's print the output. \n# Metadata is returned as a nested dictionary. Project id is listed as 'guid'\nprint(type(projectMetadata))\nprint(projectMetadata)", "execution_count": null, "outputs": []}, {"metadata": {"id": "52db350fbb9248d8912c5f87e2b8fd98"}, "cell_type": "code", "source": "# Get the project id and print it for verification\nprojectID = projectMetadata['metadata']['guid']\nprint(projectID)", "execution_count": null, "outputs": []}, {"metadata": {"id": "55be665f51a84a428ddf4d1445ef102f"}, "cell_type": "code", "source": "# Get the Job id\nproject.get_assets()", "execution_count": null, "outputs": []}, {"metadata": {"id": "054e51a95e4748d9bea08b414115fb53"}, "cell_type": "code", "source": "# Manually look up the the asset_id for the Notebook Job that you created and save it in a variable. It will be used to construct REST request URL. \n# Make sure to get the ID for the Notebook Job, not Notebook Job Run. \njobID = \"eb7731c7-4e2e-4f15-8ba7-8e5a66c83fd8\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "1c5df567ccae4a3ea199344601e44c46"}, "cell_type": "code", "source": "print(jobID)", "execution_count": null, "outputs": []}, {"metadata": {"id": "33e279a4-b633-4960-a1de-de3c40b1eecd"}, "cell_type": "code", "source": "headers = {\n     'Authorization': 'Bearer ' + token,\n     'accept': 'application/json',\n     'Content-Type': 'application/json'\n}\n\n# This JSON format will work even if the Job doesn't have parameters (like the sample notebook Job we configured in Step 1)\n\ndataDict = {\n   \"job_run\": {\n        \"configuration\": {\n            \"env_variables\": [\n                \"variable1=test1\",\n                \"variable2=test2\"\n            ]\n        }\n    }\n}\n\ndata = json.dumps(dataDict)\nprint(headers)\nprint(data)", "execution_count": null, "outputs": []}, {"metadata": {"id": "d237c6c344644eb888142ddd9eda75a4"}, "cell_type": "code", "source": "#Construct the URL for invoking the job. We are using this REST endpoint: https://cloud.ibm.com/apidocs/watson-data-api-cpd#job-runs-create\nurl =  cpd_hostname + \"/v2/jobs/\" + jobID + \"/runs?project_id=\" + projectID\nprint(url)", "execution_count": null, "outputs": []}, {"metadata": {"id": "39dec790d66347a38c68721ab1a3218c"}, "cell_type": "code", "source": "response = requests.post(url, headers=headers, data=data, verify=False)\n\nresponseContent = response.content\nprint(responseContent)", "execution_count": null, "outputs": []}, {"metadata": {"id": "bcc7226e1df647cfa16ac0d8233b7caf"}, "cell_type": "code", "source": "# If we want to check the job status, we need to get the run ID, which is called asset_id\nrunID = json.loads(responseContent)['metadata']['asset_id']\nprint(runID)", "execution_count": null, "outputs": []}, {"metadata": {"id": "d86ac301cf194fea866955769bcb574a"}, "cell_type": "code", "source": "url = cpd_hostname + \"/v2/jobs/\" + jobID + \"/runs/\" + runID + \"?project_id=\" + projectID\nprint(url)", "execution_count": null, "outputs": []}, {"metadata": {"id": "cb77f339be234c0a9097e38e2af83732"}, "cell_type": "code", "source": "response = requests.get(url, headers=headers, verify=False)\nresponseContent = response.content\nprint(responseContent)", "execution_count": null, "outputs": []}, {"metadata": {"id": "2795a54f6b93475187e6ca5fc1bf95aa"}, "cell_type": "code", "source": "# Job Status is reported in variable \"state\"\njobStatus = json.loads(responseContent)['entity']['job_run']['state']\nprint(jobStatus)", "execution_count": null, "outputs": []}, {"metadata": {"id": "6a06af01dca04e6d91967e1117906f4d"}, "cell_type": "code", "source": "# Status look up can also be implemented in a loop. This is useful when you need to invoke a 2nd job after the completion of the first one\nimport time\n\nwhile jobStatus == \"Starting\" or jobStatus == \"Running\":\n  response = requests.get(url, headers=headers, verify=False)\n  responseContent = response.content\n  jobStatus = json.loads(responseContent)['entity']['job_run']['state']\n  print(jobStatus)\n# Wait for 30 seconds before checking status again\n  time.sleep(30)", "execution_count": null, "outputs": []}, {"metadata": {"id": "e42a6d9887cf4399b36b205061573503"}, "cell_type": "code", "source": "# Here you can add the call to the 2nd step in your orchestration workflow", "execution_count": null, "outputs": []}, {"metadata": {"id": "ea828108b7304f5fbcbb0de7e2ff2fda"}, "cell_type": "markdown", "source": "**Written by: Elena Lowery, April 2021**"}, {"metadata": {"id": "ca897064-cc41-4c2b-88d4-f7392f32907c"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}